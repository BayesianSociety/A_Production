Game Overview
- Server `src/server.js:1` boots Express, mounts `/api` routes and serves `/public` plus `/uploads`; the browser app is entirely static HTML/CSS/JS with API calls for data.
- REST endpoints live in `src/routes/api.js:5`: `/questions` queries Prisma for question records (with answers/media), `/bodyparts` returns anatomical metadata, and `/attempts` validates attempt payloads with `src/lib/validate.js:3` (Zod) before persisting.
- Data model is defined in `prisma/schema.prisma:1`: questions have typed payloads (MCQ, drag-label, sort, sequence), answers, optional media, and the `Attempt` table logs kid performance metrics.
- The browser UI (`public/index.html:1`, `public/js/app.js:1`) fetches questions per topic, renders them with mode-specific renderers (MCQ, drag-label, sort, sequence), records time-to-first-answer, and posts attempts through `public/js/data-service.js:1`.
- Seed content (`scripts/seed.js:1`) provisions demo organs, organisms, and six sample questions; SQLite (`data/dev.db`) stores everything.

How an LLM Can Help
- Tutoring layer: the current feedback is static strings; an LLM can craft age-appropriate explanations, analogies, or scaffolding hints that adapt to each question type, helping kids understand *why* an answer is right/wrong.
- Conversational coach: kids could ask, “Why are lungs important?” mid-game; the LLM can answer using stored body-part facts, turning BioKids into a guided learning assistant rather than a quiz loop.
- Dynamic difficulty & remediation: by summarizing a kid’s recent `Attempt` history, the LLM can recommend next activities (e.g., stay on circulatory questions, swap to easier MCQs) or generate reflective prompts (“Tell me how the heart and lungs work together”).
- Content authoring support: teachers/admins could request “Create 3 drag-and-drop questions for digestive system ages 8‑10,” letting the LLM draft question JSON that can be reviewed before insertion.

Implementation Concept
1. **Backend hook.** Add a new router (e.g., `src/routes/ai.js`) that exposes `/api/ai/hint`, `/api/ai/chat`, etc. Each handler would gather the necessary context (question text, kid age band, attempt stats) using Prisma (`question` + `bodyPart` data) and call an LLM SDK (OpenAI, Anthropic, or a self-hosted model). Use system prompts to constrain style (“You are BioBuddy, speak to ages 5‑7 in short, supportive sentences”). Stream responses back to the client for responsiveness. Keep request payloads minimal to respect token limits: send the prompt, answer options, kid’s selected choice, and a digest of facts rather than entire DB rows.
2. **Front-end integration.** For MCQs (`public/js/app.js:62`), add a “Need a hint?” button that calls `api.getHint(question.id, attemptsSoFar)` and renders the returned text in `feedbackEl`. For drag/sort/sequence modes, the hint could describe strategies (“Match labels to where blood flows”) or reflect on mistakes. Create a small chat widget anchored to the `feedback` area where kids can type or pick canned follow-up questions, reusing the same API route.
3. **Safety & grounding.** To keep answers accurate, feed the LLM structured context: e.g., pass `bodyPart.facts` (`BodyPart.facts` JSON array) or question metadata. Optionally, run outputs through a fact-checker step or rule-based filter before showing them. Cache previously generated hints per question to reduce cost and keep consistency.
4. **Analytics-driven prompts.** When logging attempts (`src/routes/api.js:30`), also enqueue summaries to an async job that periodically asks the LLM to analyze a kid’s pattern (“struggles with sequencing, excels at MCQ”). Store that advice in a new table (e.g., `StudyPlan`) and surface it at login. This keeps expensive LLM calls off the main gameplay loop.
5. **Authoring workflow.** Build an admin-only page that allows staff to describe desired content (“Need a sort activity about healthy foods”). The backend routes the prompt plus schema examples (from existing questions) to the LLM, validates the JSON against the question schema (Zod/Prisma) before persisting, and lets humans approve. This accelerates expanding the curriculum while ensuring structural correctness.

By layering the LLM behind clear API boundaries, grounding its outputs in existing curriculum data, and keeping the front-end changes additive (new buttons/components that call the AI endpoints), you can turn BioKids from static minigames into an adaptive learning companion without touching core gameplay logic. Natural next steps would be selecting an LLM provider, designing prompt templates per question type, and prototyping the `/api/ai/hint` flow end-to-end with streaming responses.
